{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Cryptography\n",
    "\n",
    "## **Competitive Training with Proper Balance**\n",
    "\n",
    "This notebook implements a rebalanced system with proper competitive dynamics:\n",
    "\n",
    "### The Rebalancing Fixes:\n",
    "1. **Stronger Bob**: Larger architecture (512 hidden, 8 layers) for >0.8 accuracy\n",
    "2. **Weaker Eve**: Smaller architecture (24 hidden, 1 layer) to hover around 0.5\n",
    "3. **Alice Metrics**: Track Alice's entropy regulation and key dependency performance\n",
    "4. **Enhanced Shocks**: More frequent, varied shock events for better dynamics\n",
    "\n",
    "**EXPECTED RESULT: Eve → 0.5, Bob → >0.8, Alice optimized**\n",
    "\n",
    "Currently the structure of the code isn't working properly. I need to balance them out for better learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import math\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🎯 Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🚀 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "print(\"\\nREBALANCED CRYPTO System Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rebalanced Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RebalancedConfig:\n",
    "    MESSAGE_LENGTH = 32\n",
    "    KEY_LENGTH = 32\n",
    "    BATCH_SIZE = 128\n",
    "    \n",
    "    ALICE_HIDDEN_SIZE = 256\n",
    "    BOB_HIDDEN_SIZE = 512          \n",
    "    EVE_HIDDEN_SIZE = 24           \n",
    "    ALICE_NUM_LAYERS = 6\n",
    "    BOB_NUM_LAYERS = 8             \n",
    "    EVE_NUM_LAYERS = 1             #eve is way shallower yet it outperforms by a huge margin. this really needs work\n",
    "\n",
    "    DROPOUT = 0.4\n",
    "    \n",
    "    LEARNING_RATE_ALICE = 0.005      \n",
    "    LEARNING_RATE_BOB = 0.005        \n",
    "    LEARNING_RATE_EVE = 0.0005       \n",
    "    NUM_EPOCHS = 2000 #I might just increase these to be way more\n",
    "    EVE_TRAINING_RATIO = 5           # training Eve less frequently\n",
    "    \n",
    "    # these are very agressive as of now. \n",
    "    RECONSTRUCTION_WEIGHT = 3.0      \n",
    "    ENTROPY_WEIGHT = 10.0            # need to keep entropy high\n",
    "    ADVERSARIAL_WEIGHT = 2.0         \n",
    "    \n",
    "    # more noise?\n",
    "    NOISE_STRENGTH = 0.3\n",
    "    \n",
    "    #curriculum and crypto ruegularization\n",
    "    TARGET_ENTROPY_INITIAL = 0.85     \n",
    "    TARGET_ENTROPY_MID = 0.92\n",
    "    TARGET_ENTROPY_LATE = 0.98\n",
    "    CURRICULUM_MILESTONES = [100, 300] \n",
    "    \n",
    "    KEY_DEPENDENCY_MARGIN = 0.35 #increase the req might help  \n",
    "    KEY_DEPENDENCY_WEIGHT = 3.0       \n",
    "    WRONG_KEY_CONFUSION_WEIGHT = 2.0  \n",
    "    ENTROPY_REG_WEIGHT = 5.0          \n",
    "\n",
    "config = RebalancedConfig()\n",
    "print(\" REBALANCED TRAINING Configuration:\")\n",
    "print(f\"  Loss Weights: ENTROPY_REG={config.ENTROPY_REG_WEIGHT}, Adv={config.ADVERSARIAL_WEIGHT}, KeyDep={config.KEY_DEPENDENCY_WEIGHT}\")\n",
    "print(f\"  Eve Config: Size={config.EVE_HIDDEN_SIZE}, LR={config.LEARNING_RATE_EVE}, Ratio={config.EVE_TRAINING_RATIO} (WEAKENED)\")\n",
    "print(f\"  Bob Config: Size={config.BOB_HIDDEN_SIZE}, Layers={config.BOB_NUM_LAYERS} (STRENGTHENED)\")\n",
    "print(f\"  Key Sensitivity Target: {config.KEY_DEPENDENCY_MARGIN} (higher avalanche effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rebalanced Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticAlice(nn.Module):\n",
    "    def __init__(self, message_length, key_length, hidden_size, num_layers, dropout):\n",
    "        super(StochasticAlice, self).__init__()\n",
    "        \n",
    "        layers = [nn.Linear(message_length + key_length, hidden_size), nn.GELU()]\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_size, hidden_size), \n",
    "                nn.GELU(), \n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        layers.append(nn.Linear(hidden_size, message_length))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, plaintext, key):\n",
    "        combined = torch.cat([plaintext, key], dim=1)\n",
    "        logits = self.network(combined)\n",
    "        probs = 0.5 + 0.5 * torch.tanh(logits)\n",
    "        return probs\n",
    "\n",
    "class RobustBob(nn.Module):\n",
    "    def __init__(self, message_length, key_length, hidden_size, num_layers, dropout):\n",
    "        super(RobustBob, self).__init__()\n",
    "        \n",
    "        layers = [nn.Linear(message_length + key_length, hidden_size), nn.GELU()]\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_size, hidden_size), \n",
    "                nn.GELU(), \n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        layers.append(nn.Linear(hidden_size, message_length))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, ciphertext, key):\n",
    "        combined = torch.cat([ciphertext, key], dim=1)\n",
    "        logits = self.network(combined)\n",
    "        decrypted_probs = torch.sigmoid(logits)\n",
    "        return decrypted_probs\n",
    "\n",
    "class WeakEve(nn.Module):\n",
    "    #it's super and supposed to hover around 0.5... need to fix this \n",
    "    def __init__(self, message_length, hidden_size, num_layers, dropout):\n",
    "        super(WeakEve, self).__init__()\n",
    "        \n",
    "        layers = [nn.Linear(message_length, hidden_size), nn.ReLU()]\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        layers.extend([nn.Dropout(dropout), nn.Linear(hidden_size, 1)])\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.05)  # Even weaker\n",
    "    \n",
    "    def forward(self, ciphertext):\n",
    "        return self.network(ciphertext)\n",
    "\n",
    "print(\"Rebalanced architectures defined: Strong Bob, Weak Eve!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crypto-Regularized Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(decrypted, original):\n",
    "    return F.l1_loss(decrypted, original)\n",
    "\n",
    "def adversarial_loss(eve_scores, is_real):\n",
    "    target = torch.ones_like(eve_scores) if is_real else torch.zeros_like(eve_scores)\n",
    "    return F.binary_cross_entropy_with_logits(eve_scores, target)\n",
    "\n",
    "def bernoulli_entropy_from_probs(probs):\n",
    "    eps = 1e-6\n",
    "    p = probs.clamp(eps, 1 - eps)\n",
    "    return - (p * torch.log2(p) + (1 - p) * torch.log2(1 - p))\n",
    "\n",
    "def entropy_reg_loss(probs, target_entropy):\n",
    "    H = bernoulli_entropy_from_probs(probs)\n",
    "    return F.mse_loss(H, torch.full_like(H, target_entropy))\n",
    "\n",
    "def key_dependency_loss(alice, messages, keys, margin, device):\n",
    "    perm = torch.randperm(messages.size(0), device=device)\n",
    "    keys_shuf = keys[perm]\n",
    "    probs_orig = alice(messages, keys)\n",
    "    probs_shuf = alice(messages, keys_shuf)\n",
    "    dist = torch.mean(torch.abs(probs_orig - probs_shuf))\n",
    "    return F.relu(margin - dist)\n",
    "\n",
    "def wrong_key_confusion_loss(bob, ciphertext, keys, device):\n",
    "    perm = torch.randperm(ciphertext.size(0), device=device)\n",
    "    wrong_keys = keys[perm]\n",
    "    dec_wrong = bob(ciphertext, wrong_keys)\n",
    "    return F.mse_loss(dec_wrong, torch.full_like(dec_wrong, 0.5))\n",
    "\n",
    "def calculate_entropy(data_tensor):\n",
    "    p1 = data_tensor.mean().item()\n",
    "    p0 = 1 - p1\n",
    "    if p0 == 0 or p1 == 0:\n",
    "        return 0.0\n",
    "    return - (p0 * math.log2(p0) + p1 * math.log2(p1))\n",
    "\n",
    "def generate_random_data(batch_size, message_length, key_length, device):\n",
    "    messages = torch.randint(0, 2, size=(batch_size, message_length), device=device).float()\n",
    "    keys = torch.randint(0, 2, size=(batch_size, key_length), device=device).float()\n",
    "    return messages, keys\n",
    "\n",
    "print(\"Crypto-regularized loss functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rebalanced Training Loop with Alice Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = StochasticAlice(\n",
    "    config.MESSAGE_LENGTH, config.KEY_LENGTH, config.ALICE_HIDDEN_SIZE, \n",
    "    config.ALICE_NUM_LAYERS, config.DROPOUT\n",
    ").to(device)\n",
    "\n",
    "bob = RobustBob(\n",
    "    config.MESSAGE_LENGTH, config.KEY_LENGTH, config.BOB_HIDDEN_SIZE, \n",
    "    config.BOB_NUM_LAYERS, config.DROPOUT\n",
    ").to(device)\n",
    "\n",
    "eve = WeakEve(\n",
    "    config.MESSAGE_LENGTH, config.EVE_HIDDEN_SIZE, \n",
    "    config.EVE_NUM_LAYERS, config.DROPOUT\n",
    ").to(device)\n",
    "\n",
    "alice_bob_optimizer = optim.Adam(list(alice.parameters()) + list(bob.parameters()), lr=config.LEARNING_RATE_ALICE, weight_decay=1e-4)\n",
    "eve_optimizer = optim.Adam(eve.parameters(), lr=config.LEARNING_RATE_EVE, weight_decay=1e-4)\n",
    "\n",
    "alice_bob_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(alice_bob_optimizer, T_0=200, T_mult=2)\n",
    "eve_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(eve_optimizer, T_0=150, T_mult=2)\n",
    "\n",
    "history = defaultdict(list)\n",
    "\n",
    "def train_rebalanced(num_epochs):\n",
    "    print(\"STARTING REBALANCED TRAINING\")\n",
    "    print(f\"Target: Eve→0.5, Bob→>0.8, Alice optimized. Enhanced shocks every 50 epochs.\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    def get_curriculum(epoch):\n",
    "        shock_event = (epoch % 50 == 0 and epoch > 0)\n",
    "        \n",
    "        if shock_event:\n",
    "            shock_type = (epoch // 50) % 3\n",
    "            if shock_type == 0:\n",
    "                return 0.6, 1    \n",
    "            elif shock_type == 1:\n",
    "                return 0.99, 1   \n",
    "            else:\n",
    "                return 0.8, 1    \n",
    "        \n",
    "        if epoch < config.CURRICULUM_MILESTONES[0]:\n",
    "            return config.TARGET_ENTROPY_INITIAL, 5\n",
    "        elif epoch < config.CURRICULUM_MILESTONES[1]:\n",
    "            return config.TARGET_ENTROPY_MID, 3\n",
    "        else:\n",
    "            return config.TARGET_ENTROPY_LATE, 2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        target_H, eve_ratio = get_curriculum(epoch)\n",
    "        \n",
    "        alice_bob_optimizer.zero_grad()\n",
    "        \n",
    "        messages, keys = generate_random_data(config.BATCH_SIZE, config.MESSAGE_LENGTH, config.KEY_LENGTH, device)\n",
    "        \n",
    "        cipher_probs = alice(messages, keys)\n",
    "        ciphertext = torch.bernoulli(cipher_probs)\n",
    "        decrypted_probs = bob(ciphertext.detach(), keys)\n",
    "        eve_scores = eve(ciphertext.detach())\n",
    "        \n",
    "        loss_recon = reconstruction_loss(decrypted_probs, messages)\n",
    "        loss_entropy_reg = entropy_reg_loss(cipher_probs, target_H)\n",
    "        loss_keydep = key_dependency_loss(alice, messages, keys, config.KEY_DEPENDENCY_MARGIN, device)\n",
    "        loss_wrongkey = wrong_key_confusion_loss(bob, ciphertext.detach(), keys, device)\n",
    "        loss_adv = adversarial_loss(eve_scores, is_real=True)\n",
    "        \n",
    "        total_loss = (config.RECONSTRUCTION_WEIGHT * loss_recon + \n",
    "                      config.ENTROPY_REG_WEIGHT * loss_entropy_reg + \n",
    "                      config.KEY_DEPENDENCY_WEIGHT * loss_keydep + \n",
    "                      config.WRONG_KEY_CONFUSION_WEIGHT * loss_wrongkey + \n",
    "                      config.ADVERSARIAL_WEIGHT * loss_adv)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        alice_bob_optimizer.step()\n",
    "        alice_bob_scheduler.step()\n",
    "        \n",
    "        shock_event = (epoch % 50 == 0 and epoch > 0)\n",
    "        if epoch % eve_ratio == 0:\n",
    "            eve_optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                messages_eve, keys_eve = generate_random_data(config.BATCH_SIZE, config.MESSAGE_LENGTH, config.KEY_LENGTH, device)\n",
    "                probs_eve = alice(messages_eve, keys_eve)\n",
    "                ciphertext_eve = torch.bernoulli(probs_eve)\n",
    "                random_bits = torch.randint(0, 2, size=ciphertext_eve.shape, device=device).float()\n",
    "            \n",
    "            real_scores = eve(ciphertext_eve)\n",
    "            fake_scores = eve(random_bits)\n",
    "            loss_real = adversarial_loss(real_scores, True)\n",
    "            loss_fake = adversarial_loss(fake_scores, False)\n",
    "            eve_loss = (loss_real + loss_fake) / 2\n",
    "            \n",
    "            if shock_event:\n",
    "                shock_type = (epoch // 50) % 3\n",
    "                if shock_type == 0:\n",
    "                    eve_loss = eve_loss * 5.0  \n",
    "                elif shock_type == 1:\n",
    "                    eve_loss = eve_loss * 0.1  \n",
    "                else:\n",
    "                    eve_loss = eve_loss * 2.0  \n",
    "            \n",
    "            eve_loss.backward()\n",
    "            eve_optimizer.step()\n",
    "            eve_scheduler.step()\n",
    "        else:\n",
    "            eve_loss = torch.tensor(0.0)\n",
    "        \n",
    "        if epoch % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                eval_messages, eval_keys = generate_random_data(1000, config.MESSAGE_LENGTH, config.KEY_LENGTH, device)\n",
    "                eval_probs = alice(eval_messages, eval_keys)\n",
    "                eval_cipher = torch.bernoulli(eval_probs)\n",
    "                eval_decrypted = bob(eval_cipher, eval_keys)\n",
    "                eval_random = torch.randint(0, 2, size=eval_cipher.shape, device=device).float()\n",
    "                \n",
    "                bob_accuracy = 1 - reconstruction_loss((eval_decrypted > 0.5).float(), eval_messages).item()\n",
    "                entropy = calculate_entropy(eval_cipher)\n",
    "                \n",
    "                real_preds = (torch.sigmoid(eve(eval_cipher)) > 0.5).float()\n",
    "                fake_preds = (torch.sigmoid(eve(eval_random)) < 0.5).float()\n",
    "                eve_accuracy = (real_preds.mean().item() + fake_preds.mean().item()) / 2\n",
    "                \n",
    "                alice_entropy_loss_val = loss_entropy_reg.item()\n",
    "                alice_keydep_loss_val = loss_keydep.item()\n",
    "                alice_consistency = torch.std(eval_probs, dim=0).mean().item()\n",
    "                \n",
    "                perm = torch.randperm(eval_messages.size(0), device=device)\n",
    "                eval_keys_shuf = eval_keys[perm]\n",
    "                eval_probs_shuf = alice(eval_messages, eval_keys_shuf)\n",
    "                key_sensitivity = torch.mean(torch.abs(eval_probs - eval_probs_shuf)).item()\n",
    "                \n",
    "                history['bob_accuracy'].append(bob_accuracy)\n",
    "                history['entropy'].append(entropy)\n",
    "                history['eve_accuracy'].append(eve_accuracy)\n",
    "                history['alice_entropy_loss'].append(alice_entropy_loss_val)\n",
    "                history['alice_keydep_loss'].append(alice_keydep_loss_val)\n",
    "                history['alice_consistency'].append(alice_consistency)\n",
    "                history['key_sensitivity'].append(key_sensitivity)\n",
    "                history['total_loss'].append(total_loss.item())\n",
    "                history['recon_loss'].append(loss_recon.item())\n",
    "                history['entropy_reg_loss'].append(loss_entropy_reg.item())\n",
    "                history['keydep_loss'].append(loss_keydep.item())\n",
    "                history['wrongkey_loss'].append(loss_wrongkey.item())\n",
    "                history['adv_loss'].append(loss_adv.item())\n",
    "                history['epoch'].append(epoch)\n",
    "                \n",
    "                alice_lr = alice_bob_scheduler.get_last_lr()[0]\n",
    "                eve_lr = eve_scheduler.get_last_lr()[0]\n",
    "                shock_event_log = (epoch % 50 == 0 and epoch > 0)\n",
    "                shock_indicator = \"\"\n",
    "                if shock_event_log:\n",
    "                    shock_type = (epoch // 50) % 3\n",
    "                    shock_names = ['LOW-H', 'HIGH-H', 'MID-H']\n",
    "                    shock_indicator = f\" [SHOCK-{shock_names[shock_type]}!]\"\n",
    "                \n",
    "                print(f\"Epoch {epoch:4d} | H={entropy:.4f} | Bob={bob_accuracy:.4f} | Eve={eve_accuracy:.4f} | KeySens={key_sensitivity:.3f}{shock_indicator}\")\n",
    "                print(f\"    Alice: H-Loss={alice_entropy_loss_val:.4f} KeyDep-Loss={alice_keydep_loss_val:.4f} | LRs: A/B={alice_lr:.5f} E={eve_lr:.5f}\")\n",
    "                print(f\"    Target-H={target_H:.3f} | EveRatio={eve_ratio} | Consistency={alice_consistency:.3f}\")\n",
    "                print()\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"Rebalanced training loop ready with Alice metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch Training & Enhanced Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_rebalanced(config.NUM_EPOCHS)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "fig.suptitle('Rebalanced Training Results - Strong Bob, Weak Eve, Optimized Alice', fontsize=16)\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['epoch'], history['entropy'], color='red', linewidth=3)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', label='Perfect (1.0)')\n",
    "ax.set_title('Ciphertext Entropy', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['epoch'], history['bob_accuracy'], color='green', linewidth=2)\n",
    "ax.axhline(y=0.8, color='green', linestyle='--', label='Target (0.8)')\n",
    "ax.set_title('Strong Bob Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[0, 2]\n",
    "ax.plot(history['epoch'], history['eve_accuracy'], color='orange', linewidth=2)\n",
    "ax.axhline(y=0.5, color='orange', linestyle='--', label='Target (0.5)')\n",
    "ax.set_title('Weak Eve Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['epoch'], history['key_sensitivity'], color='purple', linewidth=2)\n",
    "ax.axhline(y=config.KEY_DEPENDENCY_MARGIN, color='purple', linestyle='--', label=f'Target: {config.KEY_DEPENDENCY_MARGIN}')\n",
    "ax.set_title('Key Sensitivity (Avalanche)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Key Sensitivity')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['epoch'], history['alice_entropy_loss'], color='blue', linewidth=2)\n",
    "ax.set_title('Alice Entropy Regulation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Entropy Loss')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "\n",
    "ax = axes[1, 2]\n",
    "ax.plot(history['epoch'], history['alice_keydep_loss'], color='darkblue', linewidth=2)\n",
    "ax.set_title('Alice Key Dependency')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Key Dep Loss')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(history['epoch'], history['recon_loss'], label='Reconstruction', alpha=0.8)\n",
    "ax.plot(history['epoch'], history['entropy_reg_loss'], label='Entropy Reg', linewidth=2, alpha=0.8)\n",
    "ax.plot(history['epoch'], history['keydep_loss'], label='Key Dependency', alpha=0.8)\n",
    "ax.plot(history['epoch'], history['wrongkey_loss'], label='Wrong Key', alpha=0.8)\n",
    "ax.plot(history['epoch'], history['adv_loss'], label='Adversarial', alpha=0.8)\n",
    "ax.set_title('Loss Components')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(history['epoch'], history['alice_consistency'], color='cyan', linewidth=2)\n",
    "ax.set_title('Alice Output Consistency')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Std Dev of Probs')\n",
    "ax.grid(True)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(history['epoch'], history['bob_accuracy'], label='Bob (Target >0.8)', color='green', linewidth=2)\n",
    "ax.plot(history['epoch'], history['eve_accuracy'], label='Eve (Target ~0.5)', color='orange', linewidth=2)\n",
    "ax.plot(history['epoch'], history['entropy'], label='Entropy (Target ~1.0)', color='red', linewidth=2)\n",
    "ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.set_title('Performance Summary')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Rebalanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL REBALANCED TRAINING EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_messages, final_keys = generate_random_data(5000, config.MESSAGE_LENGTH, config.KEY_LENGTH, device)\n",
    "    final_probs = alice(final_messages, final_keys)\n",
    "    final_cipher = torch.bernoulli(final_probs)\n",
    "    final_decrypted = bob(final_cipher, final_keys)\n",
    "    final_random = torch.randint(0, 2, size=final_cipher.shape, device=device).float()\n",
    "    \n",
    "    final_bob_acc = 1 - reconstruction_loss((final_decrypted > 0.5).float(), final_messages).item()\n",
    "    final_entropy = calculate_entropy(final_cipher)\n",
    "    \n",
    "    final_real_preds = (torch.sigmoid(eve(final_cipher)) > 0.5).float()\n",
    "    final_fake_preds = (torch.sigmoid(eve(final_random)) < 0.5).float()\n",
    "    final_eve_acc = (final_real_preds.mean().item() + final_fake_preds.mean().item()) / 2\n",
    "    \n",
    "    perm = torch.randperm(final_messages.size(0), device=device)\n",
    "    final_keys_shuf = final_keys[perm]\n",
    "    final_probs_shuf = alice(final_messages, final_keys_shuf)\n",
    "    final_key_sensitivity = torch.mean(torch.abs(final_probs - final_probs_shuf)).item()\n",
    "    \n",
    "    final_alice_consistency = torch.std(final_probs, dim=0).mean().item()\n",
    "\n",
    "print(f\"FINAL REBALANCED PERFORMANCE:\")\n",
    "print(f\"  **Ciphertext Entropy**: {final_entropy:.4f} (Target: ~1.0)\")\n",
    "print(f\"  **Bob's Accuracy**: {final_bob_acc:.4f} (Target: >0.8) {'✅ SUCCESS' if final_bob_acc > 0.8 else '❌ NEEDS WORK'}\")\n",
    "print(f\"  **Eve's Accuracy**: {final_eve_acc:.4f} (Target: ~0.5) {'✅ SUCCESS' if abs(final_eve_acc - 0.5) < 0.1 else '❌ TOO STRONG'}\")\n",
    "print(f\"  **Key Sensitivity**: {final_key_sensitivity:.4f} (Target: >{config.KEY_DEPENDENCY_MARGIN}) {'✅ SUCCESS' if final_key_sensitivity > config.KEY_DEPENDENCY_MARGIN else '❌ TOO LOW'}\")\n",
    "print(f\"  **Alice Consistency**: {final_alice_consistency:.4f}\")\n",
    "\n",
    "print(\"\\nREBALANCING VERDICT:\")\n",
    "success_count = 0\n",
    "if final_entropy > 0.95:\n",
    "    success_count += 1\n",
    "if final_bob_acc > 0.8:\n",
    "    success_count += 1\n",
    "if abs(final_eve_acc - 0.5) < 0.1:\n",
    "    success_count += 1\n",
    "if final_key_sensitivity > config.KEY_DEPENDENCY_MARGIN:\n",
    "    success_count += 1\n",
    "\n",
    "if success_count >= 3:\n",
    "    print(\"  **REBALANCING SUCCESS!** The system now has proper competitive dynamics!\")\n",
    "elif success_count >= 2:\n",
    "    print(\"  **Good Progress.** Most targets achieved, fine-tuning may be needed.\")\n",
    "else:\n",
    "    print(\"  **Needs More Rebalancing.** Consider further architectural or hyperparameter adjustments.\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"   • Bob (stronger): {config.BOB_HIDDEN_SIZE} hidden, {config.BOB_NUM_LAYERS} layers\")\n",
    "print(f\"   • Eve (weaker): {config.EVE_HIDDEN_SIZE} hidden, {config.EVE_NUM_LAYERS} layer, LR={config.LEARNING_RATE_EVE}\")\n",
    "print(f\"   • Enhanced shocks every 50 epochs with varied intensity\")\n",
    "print(f\"   • Alice metrics now tracked for crypto optimization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
